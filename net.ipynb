{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable   \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import linecache\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from tqdm import trange\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "torch.cuda.set_device(2)\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_show(line_batch, text=None):\n",
    "    batch = line_batch[\"line\"][0].shape[0]\n",
    "    line1 = line_batch[\"line\"][0].numpy().reshape(batch,-1)\n",
    "    line2 = line_batch[\"line\"][1].numpy().reshape(batch,-1)\n",
    "    label = line_batch[\"label\"].numpy()\n",
    "    for i in range(batch):\n",
    "        ax = plt.subplot(batch/2, 2, i+1)\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=1.5)\n",
    "        plt.plot(line1[i])\n",
    "        plt.plot(line2[i])\n",
    "        plt.axis\n",
    "        if text:\n",
    "            ax.set_title(text+str(label[i]),fontsize=12,color='r')\n",
    "    \n",
    "def line_show_test(line_batch, text=None):\n",
    "    line1 = line_batch[0].numpy().reshape(1,-1)\n",
    "    line2 = line_batch[1].numpy().reshape(1,-1)\n",
    "    plt.figure()\n",
    "    plt.plot(line1[0])\n",
    "    plt.plot(line2[0])\n",
    "    if text:\n",
    "        plt.title(text,fontsize='large',fontweight='bold') \n",
    "        \n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def read_data_row(path, num):       \n",
    "    return linecache.getline(path,num)\n",
    "\n",
    "def normalization(l1, l2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"./data/train/\"\n",
    "    testing_dir = \"./data/test/\"\n",
    "    train_val_dir = \"./data/train_val/\"\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 30\n",
    "    train_lr = 0.001\n",
    "    train_m = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkylineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,root,transform=None):\n",
    "        self.root_path = root\n",
    "        self.transform = transform\n",
    "        self.filenames = os.listdir(self.root_path)\n",
    "    def __getitem__(self,idx):\n",
    "        filename = self.filenames[idx]\n",
    "        file = read_data_row(os.path.join(self.root_path, filename), 1).strip().split(\" \")\n",
    "        line1 = np.array(list(map(int, file[0].split(',')))) # C*H*W = (1, 320, 1)\n",
    "        if self.transform:\n",
    "            line1 = Move()(line1)\n",
    "            line1 = Rotate()(line1)\n",
    "        line2 = np.array(list(map(int, file[1].split(','))))\n",
    "        line = np.hstack((line1, line2))\n",
    "        \n",
    "        line_min, line_max = line.min(), line.max()\n",
    "        line = (line-line_min)/(line_max-line_min)\n",
    "        \n",
    "        line1 = line[:320].reshape(1, 320, 1)\n",
    "        line2 = line[320:].reshape(1, 320, 1)\n",
    "        label = np.array(list(map(int, file[2])))\n",
    "        sample = {\"line\":[line1, line2], \"label\":label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotate(object):\n",
    "    def __call__(self, line):\n",
    "        angle = random.randint(-6,6)\n",
    "        pointx = len(line)//2\n",
    "        pointy = line[pointx]\n",
    "        angle = float(angle) * 3.1415  / float(180)\n",
    "        x = np.arange(len(line))\n",
    "        y = (x-pointx) * math.sin(angle) + (line - pointy) * math.cos(angle) + pointy\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Move(object):\n",
    "    def __call__(self, line):\n",
    "        delta = random.randint(-30,30)\n",
    "        return line + delta\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        line, lable = sample['line'], sample['label']\n",
    "        return {'line': [torch.from_numpy(line[0]).float(), torch.from_numpy(line[1]).float()],\n",
    "                'label': torch.from_numpy(lable).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "skyline_dataset = SkylineDataset(root=Config.training_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(skyline_dataset, \n",
    "                       shuffle=True,\n",
    "                       batch_size = 2,\n",
    "                       num_workers = 0\n",
    "                       )\n",
    "dataiter = iter(train_dataloader)\n",
    "example_batch = next(dataiter)\n",
    "line_show(example_batch, text=\"label: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "              \n",
    "        self.feature0 = nn.Sequential(\n",
    "            nn.Conv1d(1, 48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 48*160\n",
    "            \n",
    "            nn.Conv1d(48, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 128*80\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2) # 1*256*40\n",
    "        )\n",
    "        \n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 48, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 48*160\n",
    "            \n",
    "            nn.Conv1d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 128*80\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2) # 1*256*40\n",
    "        )\n",
    "        \n",
    "                \n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Conv1d(512*2, 24, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # N*24*20\n",
    "            \n",
    "             nn.Conv1d(24, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # N*8*10\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(80, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x0 = self.feature0(x)\n",
    "        x1 = self.feature1(x)\n",
    "        return torch.cat((x0,x1), 1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "\n",
    "        output = self.classify(output)\n",
    "        output = output.view(-1, 1, 80).contiguous()\n",
    "\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "dummy_input0 = torch.rand(32,1, 320) \n",
    "dummy_input1 = torch.rand(32,1, 320) \n",
    "model = SiameseNetwork()\n",
    "with SummaryWriter(comment='SiameseNetwork') as w:\n",
    "    w.add_graph(model, (dummy_input0, dummy_input1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        euclidean_distance = F.threshold(euclidean_distance, 0.1, 0.0, inplace=False)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)) # 所有batch的loss\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyline_dataset = SkylineDataset(root=Config.training_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n",
    "train_dataloader = DataLoader(skyline_dataset, \n",
    "                       shuffle=True,\n",
    "                       batch_size = Config.train_batch_size,\n",
    "                       num_workers = 0\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = Config.train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        line1 = data[\"line\"][0].view(Config.train_batch_size, 1, -1)\n",
    "        line2 = data[\"line\"][1].view(Config.train_batch_size, 1, -1)\n",
    "        label = data[\"label\"]\n",
    "        line1, line2 , label = line1.to(device), line2.to(device) , label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(line1,line2)\n",
    "        loss_contrastive = criterion(output,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i %100 == 0 :\n",
    "            print(\"Epoch {} | {} | Current loss {} \".format(epoch, i, loss_contrastive.item()))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, path, save=True):\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        filenames = os.listdir(path)\n",
    "        acc_top20 = 0\n",
    "        acc_top50 = 0\n",
    "        acc_top100 = 0\n",
    "        acc_top200 = 0\n",
    "        \n",
    "        for _ in range(5):\n",
    "            for i in trange(100):\n",
    "                index_target = random.randint(0, len(filenames)-1)\n",
    "                file0 = read_data_row(os.path.join(path, filenames[index_target]), 1).strip().split(\" \")\n",
    "                line0 = np.array(list(map(int, file0[0].split(','))))\n",
    "                dist = []\n",
    "                for f in filenames:\n",
    "                    file1 = read_data_row(os.path.join(path, f), 1).strip().split(\" \")\n",
    "                    line1 = np.array(list(map(int, file1[1].split(','))))\n",
    "                    line = np.hstack((line0, line1))\n",
    "                    line_min, line_max = line.min(), line.max()\n",
    "                    line = (line-line_min)/(line_max-line_min)\n",
    "                    line1 = torch.from_numpy(line[:320].reshape(1, 1, -1))\n",
    "                    line2 = torch.from_numpy(line[320:].reshape(1, 1, -1))\n",
    "\n",
    "                    output = net(Variable(line1).float().to(device), Variable(line2).float().to(device))\n",
    "                    dist.append(output.item())\n",
    "\n",
    "                dist = pd.Series(np.array(dist)).sort_values(ascending=False)\n",
    "                if index_target in dist[:20].index.tolist():\n",
    "                    acc_top20 += 1\n",
    "                if index_target in dist[:50].index.tolist():\n",
    "                    acc_top50 += 1    \n",
    "                if index_target in dist[:100].index.tolist():\n",
    "                    acc_top100 += 1\n",
    "                if index_target in dist[:200].index.tolist():\n",
    "                    acc_top200 += 1\n",
    "                \n",
    "        acc_top20 /= 500.0\n",
    "        acc_top50 /= 500.0\n",
    "        acc_top100 /= 500.0\n",
    "        acc_top200 /= 500.0\n",
    "        acc_mean = (acc_top20+acc_top50+acc_top100+acc_top200) / 4\n",
    "        print('epoch %d | acc_top20: %.3f | acc_top50: %.3f | acc_top100: %.3f | acc_top200: %.3f | acc_mean: %.3f' % (epoch, \n",
    "                                                                                                    acc_top20, \n",
    "                                                                                                    acc_top50,\n",
    "                                                                                                    acc_top100,\n",
    "                                                                                                    acc_top200,\n",
    "                                                                                                    acc_mean))\n",
    "        \n",
    "        # saving\n",
    "        if save:\n",
    "            global best_acc\n",
    "            if acc_mean > best_acc:\n",
    "                print(\"saving...\")\n",
    "                if not os.path.isdir('./checkpoint'):\n",
    "                    os.mkdir('./checkpoint')\n",
    "                time_str = time.strftime(\"%Y-%m-%d %X\",time.localtime())\n",
    "                torch.save(net.state_dict(), './checkpoint/ckpt.{}.{:.2f}.pth'.format(time_str, acc_mean))\n",
    "                best_acc = acc_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(0,15):\n",
    "    train(epoch)\n",
    "    test(epoch, Config.train_val_dir, save=False)\n",
    "    test(epoch, Config.testing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"./checkpoint/ckpt.2019-04-24 18:24:29.0.96.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line probability show\n",
    "folder_dataset_test = SkylineDataset(root=Config.testing_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n",
    "test_dataloader = DataLoader(folder_dataset_test,num_workers=6,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "example0 = next(dataiter)\n",
    "line0 = example0[\"line\"][0].view(1, 1, -1)\n",
    "line1 = example0[\"line\"][1].view(1, 1, -1)\n",
    "label = example0[\"label\"].numpy()\n",
    "print(label)\n",
    "for i in range(10):\n",
    "    output = model(Variable(line0).to(device),Variable(line1).to(device))\n",
    "    line_show_test((line0, line1, label),'probability: {:.8f}'.format(output.item()))\n",
    "    \n",
    "    example1 = next(dataiter)\n",
    "    line1 = example1[\"line\"][1].view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc inference\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    filenames = os.listdir(Config.testing_dir)\n",
    "    acc_top5 = 0\n",
    "    acc_top10 = 0\n",
    "    acc_top20 = 0\n",
    "    acc_top50 = 0\n",
    "    acc_top100 = 0\n",
    "    acc_top200 = 0\n",
    "\n",
    "    for i in trange(100):\n",
    "        index_target = random.randint(0, len(filenames)-1)\n",
    "        file0 = read_data_row(os.path.join(Config.testing_dir, filenames[index_target]), 1).strip().split(\" \")\n",
    "        line0 = np.array(list(map(int, file0[0].split(','))))\n",
    "        dist = []\n",
    "        for f in filenames:\n",
    "            file1 = read_data_row(os.path.join(Config.testing_dir, f), 1).strip().split(\" \")\n",
    "            line1 = np.array(list(map(int, file1[1].split(','))))\n",
    "            line = np.hstack((line0, line1))\n",
    "            line_min, line_max = line.min(), line.max()\n",
    "            line = (line-line_min)/(line_max-line_min)\n",
    "            line1 = torch.from_numpy(line[:300].reshape(1, 1, -1))\n",
    "            line2 = torch.from_numpy(line[300:].reshape(1, 1, -1))\n",
    "\n",
    "            output = model(Variable(line1).float().to(device), Variable(line2).float().to(device))\n",
    "            dist.append(output.item())\n",
    "\n",
    "        dist = pd.Series(np.array(dist)).sort_values(ascending=False)\n",
    "        if i == 0:\n",
    "            print(index_target)\n",
    "            print(dist)\n",
    "        if index_target in dist[:5].index.tolist():\n",
    "            acc_top5 += 1\n",
    "        if index_target in dist[:10].index.tolist():\n",
    "            acc_top10 += 1    \n",
    "        if index_target in dist[:20].index.tolist():\n",
    "            acc_top20 += 1\n",
    "        if index_target in dist[:50].index.tolist():\n",
    "            acc_top50 += 1    \n",
    "        if index_target in dist[:100].index.tolist():\n",
    "            acc_top100 += 1\n",
    "        if index_target in dist[:200].index.tolist():\n",
    "            acc_top200 += 1\n",
    "\n",
    "    \n",
    "    acc_top5 /= 100.0\n",
    "    acc_top10 /= 100.0\n",
    "    acc_top20 /= 100.0\n",
    "    acc_top50 /= 100.0\n",
    "    acc_top100 /= 100.0\n",
    "    acc_top200 /= 100.0\n",
    "    acc_mean = (acc_top5+acc_top10+acc_top20+acc_top50+acc_top100+acc_top200) / 6\n",
    "    pf = 'acc_top5: %.3f |acc_top10: %.3f |acc_top20: %.3f | acc_top50: %.3f | acc_top100: %.3f | acc_top200: %.3f | acc_mean: %.3f'\n",
    "    print(pf % ( acc_top5,\n",
    "                 acc_top10,\n",
    "                 acc_top20, \n",
    "                 acc_top50,\n",
    "                 acc_top100,\n",
    "                 acc_top200,\n",
    "                 acc_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
