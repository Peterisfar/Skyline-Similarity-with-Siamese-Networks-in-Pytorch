{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable   \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import linecache\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from tqdm import trange\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "torch.cuda.set_device(2)\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_show(line_batch, text=None):\n",
    "    batch = line_batch[\"line\"][0].shape[0]\n",
    "    line1 = line_batch[\"line\"][0].numpy().reshape(batch,-1)\n",
    "    line2 = line_batch[\"line\"][1].numpy().reshape(batch,-1)\n",
    "    label = line_batch[\"label\"].numpy()\n",
    "    for i in range(batch):\n",
    "        ax = plt.subplot(batch/2, 2, i+1)\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=1.5)\n",
    "        plt.plot(line1[i])\n",
    "        plt.plot(line2[i])\n",
    "        plt.axis\n",
    "        if text:\n",
    "            ax.set_title(text+str(label[i]),fontsize=12,color='r')\n",
    "    \n",
    "def line_show_test(line_batch, text=None):\n",
    "    line1 = line_batch[0].numpy().reshape(1,-1)\n",
    "    line2 = line_batch[1].numpy().reshape(1,-1)\n",
    "    plt.figure()\n",
    "    plt.plot(line1[0])\n",
    "    plt.plot(line2[0])\n",
    "    if text:\n",
    "        plt.title(text,fontsize='large',fontweight='bold') \n",
    "        \n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def read_data_row(path, num):       \n",
    "    return linecache.getline(path,num)\n",
    "\n",
    "def normalization(l1, l2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"./data/train/\"\n",
    "    testing_dir = \"./data/test/\"\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 30\n",
    "    train_lr = 0.001\n",
    "    train_m = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkylineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,root,transform=None):\n",
    "        self.root_path = root\n",
    "        self.transform = transform\n",
    "        self.filenames = os.listdir(self.root_path)\n",
    "    def __getitem__(self,idx):\n",
    "        filename = self.filenames[idx]\n",
    "        file = read_data_row(os.path.join(self.root_path, filename), 1).strip().split(\" \")\n",
    "        line1 = np.array(list(map(int, file[0].split(',')))) # C*H*W = (1, 320, 1)\n",
    "        if self.transform:\n",
    "            line1 = Move()(line1)\n",
    "            line1 = Rotate()(line1)\n",
    "        line2 = np.array(list(map(int, file[1].split(','))))\n",
    "        line = np.hstack((line1, line2))\n",
    "        \n",
    "        line_min, line_max = line.min(), line.max()\n",
    "        line = (line-line_min)/(line_max-line_min)\n",
    "        \n",
    "        line1 = line[:320].reshape(1, 320, 1)\n",
    "        line2 = line[320:].reshape(1, 320, 1)\n",
    "        label = np.array(list(map(int, file[2])))\n",
    "        sample = {\"line\":[line1, line2], \"label\":label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotate(object):\n",
    "    def __call__(self, line):\n",
    "        angle = random.randint(-6,6)\n",
    "        pointx = len(line)//2\n",
    "        pointy = line[pointx]\n",
    "        angle = float(angle) * 3.1415  / float(180)\n",
    "        x = np.arange(len(line))\n",
    "        y = (x-pointx) * math.sin(angle) + (line - pointy) * math.cos(angle) + pointy\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Move(object):\n",
    "    def __call__(self, line):\n",
    "        delta = random.randint(-30,30)\n",
    "        return line + delta\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        line, lable = sample['line'], sample['label']\n",
    "        return {'line': [torch.from_numpy(line[0]).float(), torch.from_numpy(line[1]).float()],\n",
    "                'label': torch.from_numpy(lable).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "skyline_dataset = SkylineDataset(root=Config.training_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(skyline_dataset, \n",
    "                       shuffle=True,\n",
    "                       batch_size = 2,\n",
    "                       num_workers = 0\n",
    "                       )\n",
    "dataiter = iter(train_dataloader)\n",
    "example_batch = next(dataiter)\n",
    "line_show(example_batch, text=\"label: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "              \n",
    "        self.feature0 = nn.Sequential(\n",
    "            nn.Conv1d(1, 48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 48*160\n",
    "            \n",
    "            nn.Conv1d(48, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 128*80\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2) # 1*256*40\n",
    "        )\n",
    "        \n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 48, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 48*160\n",
    "            \n",
    "            nn.Conv1d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # 128*80\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2) # 1*256*40\n",
    "        )\n",
    "        \n",
    "                \n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Conv1d(512*2, 24, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # N*24*20\n",
    "            \n",
    "             nn.Conv1d(24, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # N*8*10\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(80, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x0 = self.feature0(x)\n",
    "        x1 = self.feature1(x)\n",
    "        return torch.cat((x0,x1), 1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "\n",
    "        output = self.classify(output)\n",
    "        output = output.view(-1, 1, 80).contiguous()\n",
    "\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "dummy_input0 = torch.rand(32,1, 320) \n",
    "dummy_input1 = torch.rand(32,1, 320) \n",
    "model = SiameseNetwork()\n",
    "with SummaryWriter(comment='SiameseNetwork') as w:\n",
    "    w.add_graph(model, (dummy_input0, dummy_input1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        euclidean_distance = F.threshold(euclidean_distance, 0.1, 0.0, inplace=False)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)) # 所有batch的loss\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyline_dataset = SkylineDataset(root=Config.training_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n",
    "train_dataloader = DataLoader(skyline_dataset, \n",
    "                       shuffle=True,\n",
    "                       batch_size = Config.train_batch_size,\n",
    "                       num_workers = 0\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = Config.train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(echoh):\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        line1 = data[\"line\"][0].view(Config.train_batch_size, 1, -1)\n",
    "        line2 = data[\"line\"][1].view(Config.train_batch_size, 1, -1)\n",
    "        label = data[\"label\"]\n",
    "        line1, line2 , label = line1.to(device), line2.to(device) , label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(line1,line2)\n",
    "        loss_contrastive = criterion(output,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i %100 == 0 :\n",
    "            print(\"Epoch {} | {} | Current loss {} \".format(epoch, i, loss_contrastive.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        filenames = os.listdir(Config.testing_dir)\n",
    "        acc_top20 = 0\n",
    "        acc_top50 = 0\n",
    "        acc_top100 = 0\n",
    "        acc_top200 = 0\n",
    "        \n",
    "        for _ in range(5):\n",
    "            for i in trange(100):\n",
    "                index_target = random.randint(0, len(filenames)-1)\n",
    "                file0 = read_data_row(os.path.join(Config.testing_dir, filenames[index_target]), 1).strip().split(\" \")\n",
    "                line0 = np.array(list(map(int, file0[0].split(','))))\n",
    "                dist = []\n",
    "                for f in filenames:\n",
    "                    file1 = read_data_row(os.path.join(Config.testing_dir, f), 1).strip().split(\" \")\n",
    "                    line1 = np.array(list(map(int, file1[1].split(','))))\n",
    "                    line = np.hstack((line0, line1))\n",
    "                    line_min, line_max = line.min(), line.max()\n",
    "                    line = (line-line_min)/(line_max-line_min)\n",
    "                    line1 = torch.from_numpy(line[:320].reshape(1, 1, -1))\n",
    "                    line2 = torch.from_numpy(line[320:].reshape(1, 1, -1))\n",
    "\n",
    "                    output = net(Variable(line1).float().to(device), Variable(line2).float().to(device))\n",
    "                    dist.append(output.item())\n",
    "\n",
    "                dist = pd.Series(np.array(dist)).sort_values(ascending=False)\n",
    "                if index_target in dist[:20].index.tolist():\n",
    "                    acc_top20 += 1\n",
    "                if index_target in dist[:50].index.tolist():\n",
    "                    acc_top50 += 1    \n",
    "                if index_target in dist[:100].index.tolist():\n",
    "                    acc_top100 += 1\n",
    "                if index_target in dist[:200].index.tolist():\n",
    "                    acc_top200 += 1\n",
    "                \n",
    "        acc_top20 /= 500.0\n",
    "        acc_top50 /= 500.0\n",
    "        acc_top100 /= 500.0\n",
    "        acc_top200 /= 500.0\n",
    "        acc_mean = (acc_top20+acc_top50+acc_top100+acc_top200) / 4\n",
    "        print('epoch %d | acc_top20: %.3f | acc_top50: %.3f | acc_top100: %.3f | acc_top200: %.3f | acc_mean: %.3f' % (epoch, \n",
    "                                                                                                    acc_top20, \n",
    "                                                                                                    acc_top50,\n",
    "                                                                                                    acc_top100,\n",
    "                                                                                                    acc_top200,\n",
    "                                                                                                    acc_mean))\n",
    "        \n",
    "        # saving\n",
    "        global best_acc\n",
    "        if acc_mean > best_acc:\n",
    "            print(\"saving...\")\n",
    "            if not os.path.isdir('./checkpoint'):\n",
    "                os.mkdir('./checkpoint')\n",
    "            time_str = time.strftime(\"%Y-%m-%d %X\",time.localtime())\n",
    "            torch.save(net.state_dict(), './checkpoint/ckpt.{}.{:.2f}.pth'.format(time_str, acc_mean))\n",
    "            best_acc = acc_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | 0 | Current loss 0.6697843074798584 \n",
      "Epoch 0 | 100 | Current loss 0.5487669706344604 \n",
      "Epoch 0 | 200 | Current loss 0.46556079387664795 \n",
      "Epoch 0 | 300 | Current loss 0.45551204681396484 \n",
      "Epoch 0 | 400 | Current loss 0.4134024977684021 \n",
      "Epoch 0 | 500 | Current loss 0.34110069274902344 \n",
      "Epoch 0 | 600 | Current loss 0.4652709662914276 \n",
      "Epoch 0 | 700 | Current loss 0.2878352403640747 \n",
      "Epoch 0 | 800 | Current loss 0.47794440388679504 \n",
      "Epoch 0 | 900 | Current loss 0.3584999144077301 \n",
      "Epoch 0 | 1000 | Current loss 0.268300861120224 \n",
      "Epoch 0 | 1100 | Current loss 0.3451995253562927 \n",
      "Epoch 0 | 1200 | Current loss 0.30591168999671936 \n",
      "Epoch 0 | 1300 | Current loss 0.3198258876800537 \n",
      "Epoch 0 | 1400 | Current loss 0.37471961975097656 \n",
      "Epoch 0 | 1500 | Current loss 0.34812676906585693 \n",
      "Epoch 0 | 1600 | Current loss 0.37131059169769287 \n",
      "Epoch 0 | 1700 | Current loss 0.3227810561656952 \n",
      "Epoch 0 | 1800 | Current loss 0.26259806752204895 \n",
      "Epoch 0 | 1900 | Current loss 0.19853244721889496 \n",
      "Epoch 0 | 2000 | Current loss 0.3653625547885895 \n",
      "Epoch 0 | 2100 | Current loss 0.31543421745300293 \n",
      "Epoch 0 | 2200 | Current loss 0.3605723977088928 \n",
      "Epoch 0 | 2300 | Current loss 0.3641113042831421 \n",
      "Epoch 0 | 2400 | Current loss 0.4772704243659973 \n",
      "Epoch 0 | 2500 | Current loss 0.2886219620704651 \n",
      "Epoch 0 | 2600 | Current loss 0.21113485097885132 \n",
      "Epoch 0 | 2700 | Current loss 0.18485534191131592 \n",
      "Epoch 0 | 2800 | Current loss 0.2886483073234558 \n",
      "Epoch 0 | 2900 | Current loss 0.35431748628616333 \n",
      "Epoch 0 | 3000 | Current loss 0.24252530932426453 \n",
      "Epoch 0 | 3100 | Current loss 0.34416866302490234 \n",
      "Epoch 0 | 3200 | Current loss 0.2085418403148651 \n",
      "Epoch 0 | 3300 | Current loss 0.4388892650604248 \n",
      "Epoch 0 | 3400 | Current loss 0.3071531653404236 \n",
      "Epoch 0 | 3500 | Current loss 0.2898724675178528 \n",
      "Epoch 0 | 3600 | Current loss 0.25438040494918823 \n",
      "Epoch 0 | 3700 | Current loss 0.2559105455875397 \n",
      "Epoch 0 | 3800 | Current loss 0.2834537923336029 \n",
      "Epoch 0 | 3900 | Current loss 0.3504962921142578 \n",
      "Epoch 0 | 4000 | Current loss 0.2965405285358429 \n",
      "Epoch 0 | 4100 | Current loss 0.2895824909210205 \n",
      "Epoch 0 | 4200 | Current loss 0.1738033890724182 \n",
      "Epoch 0 | 4300 | Current loss 0.23605282604694366 \n",
      "Epoch 0 | 4400 | Current loss 0.2320578247308731 \n",
      "Epoch 0 | 4500 | Current loss 0.3318188190460205 \n",
      "Epoch 0 | 4600 | Current loss 0.2175719141960144 \n",
      "Epoch 0 | 4700 | Current loss 0.16117337346076965 \n",
      "Epoch 0 | 4800 | Current loss 0.6137067675590515 \n",
      "Epoch 0 | 4900 | Current loss 0.25992634892463684 \n",
      "Epoch 0 | 5000 | Current loss 0.09374608099460602 \n",
      "Epoch 0 | 5100 | Current loss 0.2156417965888977 \n",
      "Epoch 0 | 5200 | Current loss 0.1690027415752411 \n",
      "Epoch 0 | 5300 | Current loss 0.12717893719673157 \n",
      "Epoch 0 | 5400 | Current loss 0.20859548449516296 \n",
      "Epoch 0 | 5500 | Current loss 0.36427760124206543 \n",
      "Epoch 0 | 5600 | Current loss 0.21475937962532043 \n",
      "Epoch 0 | 5700 | Current loss 0.18705065548419952 \n",
      "Epoch 0 | 5800 | Current loss 0.43172594904899597 \n",
      "Epoch 0 | 5900 | Current loss 0.1488569676876068 \n",
      "Epoch 0 | 6000 | Current loss 0.2849907875061035 \n",
      "Epoch 0 | 6100 | Current loss 0.17207521200180054 \n",
      "Epoch 0 | 6200 | Current loss 0.26687806844711304 \n",
      "Epoch 0 | 6300 | Current loss 0.5208269357681274 \n",
      "Epoch 0 | 6400 | Current loss 0.19431518018245697 \n",
      "Epoch 0 | 6500 | Current loss 0.32222267985343933 \n",
      "Epoch 0 | 6600 | Current loss 0.13900898396968842 \n",
      "Epoch 0 | 6700 | Current loss 0.1463068574666977 \n",
      "Epoch 0 | 6800 | Current loss 0.21065357327461243 \n",
      "Epoch 0 | 6900 | Current loss 0.17396976053714752 \n",
      "Epoch 0 | 7000 | Current loss 0.27940133213996887 \n",
      "Epoch 0 | 7100 | Current loss 0.13533268868923187 \n",
      "Epoch 0 | 7200 | Current loss 0.09319247305393219 \n",
      "Epoch 0 | 7300 | Current loss 0.1605233997106552 \n",
      "Epoch 0 | 7400 | Current loss 0.15762892365455627 \n",
      "Epoch 0 | 7500 | Current loss 0.20120762288570404 \n",
      "Epoch 0 | 7600 | Current loss 0.1215563639998436 \n",
      "Epoch 0 | 7700 | Current loss 0.1747107207775116 \n",
      "Epoch 0 | 7800 | Current loss 0.20591846108436584 \n",
      "Epoch 0 | 7900 | Current loss 0.21947523951530457 \n",
      "Epoch 0 | 8000 | Current loss 0.26002612709999084 \n",
      "Epoch 0 | 8100 | Current loss 0.24266980588436127 \n",
      "Epoch 0 | 8200 | Current loss 0.12840807437896729 \n",
      "Epoch 0 | 8300 | Current loss 0.14970232546329498 \n",
      "Epoch 0 | 8400 | Current loss 0.213362455368042 \n",
      "Epoch 0 | 8500 | Current loss 0.4741109013557434 \n",
      "Epoch 0 | 8600 | Current loss 0.30192193388938904 \n",
      "Epoch 0 | 8700 | Current loss 0.16666679084300995 \n",
      "Epoch 0 | 8800 | Current loss 0.3275778889656067 \n",
      "Epoch 0 | 8900 | Current loss 0.23973850905895233 \n",
      "Epoch 0 | 9000 | Current loss 0.17617610096931458 \n",
      "Epoch 0 | 9100 | Current loss 0.15506552159786224 \n",
      "Epoch 0 | 9200 | Current loss 0.10410827398300171 \n",
      "Epoch 0 | 9300 | Current loss 0.21844571828842163 \n",
      "Epoch 0 | 9400 | Current loss 0.08926684409379959 \n",
      "Epoch 0 | 9500 | Current loss 0.08437320590019226 \n",
      "Epoch 0 | 9600 | Current loss 0.23761628568172455 \n",
      "Epoch 0 | 9700 | Current loss 0.24370121955871582 \n",
      "Epoch 0 | 9800 | Current loss 0.21916857361793518 \n",
      "Epoch 0 | 9900 | Current loss 0.1395224928855896 \n",
      "Epoch 0 | 10000 | Current loss 0.19858451187610626 \n",
      "Epoch 0 | 10100 | Current loss 0.13624468445777893 \n",
      "Epoch 0 | 10200 | Current loss 0.3183576464653015 \n",
      "Epoch 0 | 10300 | Current loss 0.16742272675037384 \n",
      "Epoch 0 | 10400 | Current loss 0.15217185020446777 \n",
      "Epoch 0 | 10500 | Current loss 0.16048647463321686 \n",
      "Epoch 0 | 10600 | Current loss 0.1830892562866211 \n",
      "Epoch 0 | 10700 | Current loss 0.43900731205940247 \n",
      "Epoch 0 | 10800 | Current loss 0.27639240026474 \n",
      "Epoch 0 | 10900 | Current loss 0.1641910970211029 \n",
      "Epoch 0 | 11000 | Current loss 0.1889791041612625 \n",
      "Epoch 0 | 11100 | Current loss 0.14012044668197632 \n",
      "Epoch 0 | 11200 | Current loss 0.10929196327924728 \n",
      "Epoch 0 | 11300 | Current loss 0.2850654721260071 \n",
      "Epoch 0 | 11400 | Current loss 0.10921277850866318 \n",
      "Epoch 0 | 11500 | Current loss 0.309964120388031 \n",
      "Epoch 0 | 11600 | Current loss 0.30958694219589233 \n",
      "Epoch 0 | 11700 | Current loss 0.11465810984373093 \n",
      "Epoch 0 | 11800 | Current loss 0.33666539192199707 \n",
      "Epoch 0 | 11900 | Current loss 0.18159551918506622 \n",
      "Epoch 0 | 12000 | Current loss 0.14201480150222778 \n",
      "Epoch 0 | 12100 | Current loss 0.24443422257900238 \n",
      "Epoch 0 | 12200 | Current loss 0.15283723175525665 \n",
      "Epoch 0 | 12300 | Current loss 0.1333768516778946 \n",
      "Epoch 0 | 12400 | Current loss 0.22155152261257172 \n",
      "Epoch 0 | 12500 | Current loss 0.2713163495063782 \n",
      "Epoch 0 | 12600 | Current loss 0.36640259623527527 \n",
      "Epoch 0 | 12700 | Current loss 0.20082908868789673 \n",
      "Epoch 0 | 12800 | Current loss 0.3530976176261902 \n",
      "Epoch 0 | 12900 | Current loss 0.18255715072155 \n",
      "Epoch 0 | 13000 | Current loss 0.10713379085063934 \n",
      "Epoch 0 | 13100 | Current loss 0.056099213659763336 \n",
      "Epoch 0 | 13200 | Current loss 0.13285115361213684 \n",
      "Epoch 0 | 13300 | Current loss 0.24823473393917084 \n",
      "Epoch 0 | 13400 | Current loss 0.10789228230714798 \n",
      "Epoch 0 | 13500 | Current loss 0.2265622615814209 \n",
      "Epoch 0 | 13600 | Current loss 0.30174317955970764 \n",
      "Epoch 0 | 13700 | Current loss 0.12813271582126617 \n",
      "Epoch 0 | 13800 | Current loss 0.17085842788219452 \n",
      "Epoch 0 | 13900 | Current loss 0.15205836296081543 \n",
      "Epoch 0 | 14000 | Current loss 0.130209818482399 \n",
      "Epoch 0 | 14100 | Current loss 0.14286907017230988 \n",
      "Epoch 0 | 14200 | Current loss 0.1802966147661209 \n",
      "Epoch 0 | 14300 | Current loss 0.35665467381477356 \n",
      "Epoch 0 | 14400 | Current loss 0.08738947659730911 \n",
      "Epoch 0 | 14500 | Current loss 0.18741057813167572 \n",
      "Epoch 0 | 14600 | Current loss 0.13189074397087097 \n",
      "Epoch 0 | 14700 | Current loss 0.1648494303226471 \n",
      "Epoch 0 | 14800 | Current loss 0.23968087136745453 \n",
      "Epoch 0 | 14900 | Current loss 0.07298292219638824 \n",
      "Epoch 0 | 15000 | Current loss 0.09030093997716904 \n",
      "Epoch 0 | 15100 | Current loss 0.1341075599193573 \n",
      "Epoch 0 | 15200 | Current loss 0.2914767265319824 \n",
      "Epoch 0 | 15300 | Current loss 0.13622960448265076 \n",
      "Epoch 0 | 15400 | Current loss 0.08842068910598755 \n",
      "Epoch 0 | 15500 | Current loss 0.20015941560268402 \n",
      "Epoch 0 | 15600 | Current loss 0.10003837198019028 \n",
      "Epoch 0 | 15700 | Current loss 0.1452350914478302 \n",
      "Epoch 0 | 15800 | Current loss 0.11263492703437805 \n",
      "Epoch 0 | 15900 | Current loss 0.27990177273750305 \n",
      "Epoch 0 | 16000 | Current loss 0.3874402642250061 \n",
      "Epoch 0 | 16100 | Current loss 0.2712533473968506 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | 16200 | Current loss 0.27358806133270264 \n",
      "Epoch 0 | 16300 | Current loss 0.21105201542377472 \n",
      "Epoch 0 | 16400 | Current loss 0.13750025629997253 \n",
      "Epoch 0 | 16500 | Current loss 0.0673576071858406 \n",
      "Epoch 0 | 16600 | Current loss 0.20368263125419617 \n",
      "Epoch 0 | 16700 | Current loss 0.40975409746170044 \n",
      "Epoch 0 | 16800 | Current loss 0.051595136523246765 \n",
      "Epoch 0 | 16900 | Current loss 0.07407696545124054 \n",
      "Epoch 0 | 17000 | Current loss 0.1988128125667572 \n",
      "Epoch 0 | 17100 | Current loss 0.11658503115177155 \n",
      "Epoch 0 | 17200 | Current loss 0.07878275215625763 \n",
      "Epoch 0 | 17300 | Current loss 0.3967391848564148 \n",
      "Epoch 0 | 17400 | Current loss 0.07030297815799713 \n",
      "Epoch 0 | 17500 | Current loss 0.16406787931919098 \n",
      "Epoch 0 | 17600 | Current loss 0.1573016196489334 \n",
      "Epoch 0 | 17700 | Current loss 0.1471501588821411 \n",
      "Epoch 0 | 17800 | Current loss 0.23779936134815216 \n",
      "Epoch 0 | 17900 | Current loss 0.25577521324157715 \n",
      "Epoch 0 | 18000 | Current loss 0.12418626993894577 \n",
      "Epoch 0 | 18100 | Current loss 0.14177538454532623 \n",
      "Epoch 0 | 18200 | Current loss 0.05170059949159622 \n",
      "Epoch 0 | 18300 | Current loss 0.1034662127494812 \n",
      "Epoch 0 | 18400 | Current loss 0.19911666214466095 \n",
      "Epoch 0 | 18500 | Current loss 0.14632773399353027 \n",
      "Epoch 0 | 18600 | Current loss 0.24250273406505585 \n",
      "Epoch 0 | 18700 | Current loss 0.26213619112968445 \n",
      "Epoch 0 | 18800 | Current loss 0.09293614327907562 \n",
      "Epoch 0 | 18900 | Current loss 0.13914622366428375 \n",
      "Epoch 0 | 19000 | Current loss 0.24168166518211365 \n",
      "Epoch 0 | 19100 | Current loss 0.2725980877876282 \n",
      "Epoch 0 | 19200 | Current loss 0.14101693034172058 \n",
      "Epoch 0 | 19300 | Current loss 0.07731597125530243 \n",
      "Epoch 0 | 19400 | Current loss 0.14698760211467743 \n",
      "Epoch 0 | 19500 | Current loss 0.08007534593343735 \n",
      "Epoch 0 | 19600 | Current loss 0.09627723693847656 \n",
      "Epoch 0 | 19700 | Current loss 0.11283774673938751 \n",
      "Epoch 0 | 19800 | Current loss 0.3514522612094879 \n",
      "Epoch 0 | 19900 | Current loss 0.11806932091712952 \n",
      "Epoch 0 | 20000 | Current loss 0.053504347801208496 \n",
      "Epoch 0 | 20100 | Current loss 0.06637021154165268 \n",
      "Epoch 0 | 20200 | Current loss 0.22759100794792175 \n",
      "Epoch 0 | 20300 | Current loss 0.19825905561447144 \n",
      "Epoch 0 | 20400 | Current loss 0.10851508378982544 \n",
      "Epoch 0 | 20500 | Current loss 0.17146432399749756 \n",
      "Epoch 0 | 20600 | Current loss 0.3737858831882477 \n",
      "Epoch 0 | 20700 | Current loss 0.10354353487491608 \n",
      "Epoch 0 | 20800 | Current loss 0.1259121298789978 \n",
      "Epoch 0 | 20900 | Current loss 0.10173618048429489 \n",
      "Epoch 0 | 21000 | Current loss 0.2693830132484436 \n",
      "Epoch 0 | 21100 | Current loss 0.1293007880449295 \n",
      "Epoch 0 | 21200 | Current loss 0.09238837659358978 \n",
      "Epoch 0 | 21300 | Current loss 0.12601056694984436 \n",
      "Epoch 0 | 21400 | Current loss 0.11719294637441635 \n",
      "Epoch 0 | 21500 | Current loss 0.18952667713165283 \n",
      "Epoch 0 | 21600 | Current loss 0.28210359811782837 \n",
      "Epoch 0 | 21700 | Current loss 0.18296222388744354 \n",
      "Epoch 0 | 21800 | Current loss 0.13000130653381348 \n",
      "Epoch 0 | 21900 | Current loss 0.25555479526519775 \n",
      "Epoch 0 | 22000 | Current loss 0.16808827221393585 \n",
      "Epoch 0 | 22100 | Current loss 0.18817824125289917 \n",
      "Epoch 0 | 22200 | Current loss 0.15145745873451233 \n",
      "Epoch 0 | 22300 | Current loss 0.266997754573822 \n",
      "Epoch 0 | 22400 | Current loss 0.33448541164398193 \n",
      "Epoch 0 | 22500 | Current loss 0.12443352490663528 \n",
      "Epoch 0 | 22600 | Current loss 0.09358836710453033 \n",
      "Epoch 0 | 22700 | Current loss 0.1634778380393982 \n",
      "Epoch 0 | 22800 | Current loss 0.33891284465789795 \n",
      "Epoch 0 | 22900 | Current loss 0.2851729393005371 \n",
      "Epoch 0 | 23000 | Current loss 0.23838770389556885 \n",
      "Epoch 0 | 23100 | Current loss 0.14236684143543243 \n",
      "Epoch 0 | 23200 | Current loss 0.052675340324640274 \n",
      "Epoch 0 | 23300 | Current loss 0.07895326614379883 \n",
      "Epoch 0 | 23400 | Current loss 0.10727839916944504 \n",
      "Epoch 0 | 23500 | Current loss 0.17265617847442627 \n",
      "Epoch 0 | 23600 | Current loss 0.22707942128181458 \n",
      "Epoch 0 | 23700 | Current loss 0.23202843964099884 \n",
      "Epoch 0 | 23800 | Current loss 0.12240254878997803 \n",
      "Epoch 0 | 23900 | Current loss 0.08591944724321365 \n",
      "Epoch 0 | 24000 | Current loss 0.3017481863498688 \n",
      "Epoch 0 | 24100 | Current loss 0.17819757759571075 \n",
      "Epoch 0 | 24200 | Current loss 0.0558880940079689 \n",
      "Epoch 0 | 24300 | Current loss 0.1750100702047348 \n",
      "Epoch 0 | 24400 | Current loss 0.13011950254440308 \n",
      "Epoch 0 | 24500 | Current loss 0.15584953129291534 \n",
      "Epoch 0 | 24600 | Current loss 0.19990558922290802 \n",
      "Epoch 0 | 24700 | Current loss 0.01841585338115692 \n",
      "Epoch 0 | 24800 | Current loss 0.1718921661376953 \n",
      "Epoch 0 | 24900 | Current loss 0.15345992147922516 \n",
      "Epoch 0 | 25000 | Current loss 0.20885273814201355 \n",
      "Epoch 0 | 25100 | Current loss 0.15949828922748566 \n",
      "Epoch 0 | 25200 | Current loss 0.24385042488574982 \n",
      "Epoch 0 | 25300 | Current loss 0.1242203488945961 \n",
      "Epoch 0 | 25400 | Current loss 0.060303010046482086 \n",
      "Epoch 0 | 25500 | Current loss 0.2803455591201782 \n",
      "Epoch 0 | 25600 | Current loss 0.2885628044605255 \n",
      "Epoch 0 | 25700 | Current loss 0.1672113537788391 \n",
      "Epoch 0 | 25800 | Current loss 0.2533731460571289 \n",
      "Epoch 0 | 25900 | Current loss 0.136563241481781 \n",
      "Epoch 0 | 26000 | Current loss 0.11329428851604462 \n",
      "Epoch 0 | 26100 | Current loss 0.20428583025932312 \n",
      "Epoch 0 | 26200 | Current loss 0.17053239047527313 \n",
      "Epoch 0 | 26300 | Current loss 0.23970156908035278 \n",
      "Epoch 0 | 26400 | Current loss 0.07400164008140564 \n",
      "Epoch 0 | 26500 | Current loss 0.1574399471282959 \n",
      "Epoch 0 | 26600 | Current loss 0.08608782291412354 \n",
      "Epoch 0 | 26700 | Current loss 0.17822206020355225 \n",
      "Epoch 0 | 26800 | Current loss 0.24060729146003723 \n",
      "Epoch 0 | 26900 | Current loss 0.08081825077533722 \n",
      "Epoch 0 | 27000 | Current loss 0.10186274349689484 \n",
      "Epoch 0 | 27100 | Current loss 0.10545884817838669 \n",
      "Epoch 0 | 27200 | Current loss 0.05912315845489502 \n",
      "Epoch 0 | 27300 | Current loss 0.2392444908618927 \n",
      "Epoch 0 | 27400 | Current loss 0.1059311181306839 \n",
      "Epoch 0 | 27500 | Current loss 0.1494729220867157 \n",
      "Epoch 0 | 27600 | Current loss 0.1129152700304985 \n",
      "Epoch 0 | 27700 | Current loss 0.10054473578929901 \n",
      "Epoch 0 | 27800 | Current loss 0.20805007219314575 \n",
      "Epoch 0 | 27900 | Current loss 0.1271892488002777 \n",
      "Epoch 0 | 28000 | Current loss 0.13763128221035004 \n",
      "Epoch 0 | 28100 | Current loss 0.21316051483154297 \n",
      "Epoch 0 | 28200 | Current loss 0.14913859963417053 \n",
      "Epoch 0 | 28300 | Current loss 0.1875307261943817 \n",
      "Epoch 0 | 28400 | Current loss 0.11611747741699219 \n",
      "Epoch 0 | 28500 | Current loss 0.1308271288871765 \n",
      "Epoch 0 | 28600 | Current loss 0.13388702273368835 \n",
      "Epoch 0 | 28700 | Current loss 0.2137630134820938 \n",
      "Epoch 0 | 28800 | Current loss 0.061977650970220566 \n",
      "Epoch 0 | 28900 | Current loss 0.06744183599948883 \n",
      "Epoch 0 | 29000 | Current loss 0.1600867211818695 \n",
      "Epoch 0 | 29100 | Current loss 0.2062472105026245 \n",
      "Epoch 0 | 29200 | Current loss 0.11712726950645447 \n",
      "Epoch 0 | 29300 | Current loss 0.4532773494720459 \n",
      "Epoch 0 | 29400 | Current loss 0.04561068117618561 \n",
      "Epoch 0 | 29500 | Current loss 0.21193349361419678 \n",
      "Epoch 0 | 29600 | Current loss 0.09005703777074814 \n",
      "Epoch 0 | 29700 | Current loss 0.09404513239860535 \n",
      "Epoch 0 | 29800 | Current loss 0.06222768872976303 \n",
      "Epoch 0 | 29900 | Current loss 0.24609318375587463 \n",
      "Epoch 0 | 30000 | Current loss 0.0712822824716568 \n",
      "Epoch 0 | 30100 | Current loss 0.23667600750923157 \n",
      "Epoch 0 | 30200 | Current loss 0.29429084062576294 \n",
      "Epoch 0 | 30300 | Current loss 0.1206655353307724 \n",
      "Epoch 0 | 30400 | Current loss 0.12826937437057495 \n",
      "Epoch 0 | 30500 | Current loss 0.05967345088720322 \n",
      "Epoch 0 | 30600 | Current loss 0.06871272623538971 \n",
      "Epoch 0 | 30700 | Current loss 0.03448983654379845 \n",
      "Epoch 0 | 30800 | Current loss 0.20162712037563324 \n",
      "Epoch 0 | 30900 | Current loss 0.29292556643486023 \n",
      "Epoch 0 | 31000 | Current loss 0.2696223556995392 \n",
      "Epoch 0 | 31100 | Current loss 0.149302139878273 \n",
      "Epoch 0 | 31200 | Current loss 0.23296591639518738 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:41<00:00,  3.92s/it]\n",
      "100%|██████████| 100/100 [06:32<00:00,  3.94s/it]\n",
      "100%|██████████| 100/100 [06:31<00:00,  3.91s/it]\n",
      "100%|██████████| 100/100 [06:32<00:00,  3.93s/it]\n",
      "100%|██████████| 100/100 [06:33<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | acc_top20: 0.884 | acc_top50: 0.952 | acc_top100: 0.966 | acc_top200: 0.988 | acc_mean: 0.947\n",
      "saving...\n",
      "Epoch 1 | 0 | Current loss 0.24889233708381653 \n",
      "Epoch 1 | 100 | Current loss 0.13632601499557495 \n",
      "Epoch 1 | 200 | Current loss 0.16260367631912231 \n",
      "Epoch 1 | 300 | Current loss 0.18951763212680817 \n",
      "Epoch 1 | 400 | Current loss 0.07446274161338806 \n",
      "Epoch 1 | 500 | Current loss 0.1425761729478836 \n",
      "Epoch 1 | 600 | Current loss 0.2551434636116028 \n",
      "Epoch 1 | 700 | Current loss 0.09838831424713135 \n",
      "Epoch 1 | 800 | Current loss 0.11984513700008392 \n",
      "Epoch 1 | 900 | Current loss 0.19628757238388062 \n",
      "Epoch 1 | 1000 | Current loss 0.16010548174381256 \n",
      "Epoch 1 | 1100 | Current loss 0.0927603542804718 \n",
      "Epoch 1 | 1200 | Current loss 0.1055317372083664 \n",
      "Epoch 1 | 1300 | Current loss 0.05684088543057442 \n",
      "Epoch 1 | 1400 | Current loss 0.1969655454158783 \n",
      "Epoch 1 | 1500 | Current loss 0.15676164627075195 \n",
      "Epoch 1 | 1600 | Current loss 0.10288776457309723 \n",
      "Epoch 1 | 1700 | Current loss 0.22252696752548218 \n",
      "Epoch 1 | 1800 | Current loss 0.024045292288064957 \n",
      "Epoch 1 | 1900 | Current loss 0.17543888092041016 \n",
      "Epoch 1 | 2000 | Current loss 0.17218872904777527 \n",
      "Epoch 1 | 2100 | Current loss 0.3416423797607422 \n",
      "Epoch 1 | 2200 | Current loss 0.20830154418945312 \n",
      "Epoch 1 | 2300 | Current loss 0.1027509868144989 \n",
      "Epoch 1 | 2400 | Current loss 0.09952154755592346 \n",
      "Epoch 1 | 2500 | Current loss 0.08411767333745956 \n",
      "Epoch 1 | 2600 | Current loss 0.0949990376830101 \n",
      "Epoch 1 | 2700 | Current loss 0.06405758857727051 \n",
      "Epoch 1 | 2800 | Current loss 0.06351271271705627 \n",
      "Epoch 1 | 2900 | Current loss 0.20645427703857422 \n",
      "Epoch 1 | 3000 | Current loss 0.06708421558141708 \n",
      "Epoch 1 | 3100 | Current loss 0.15161734819412231 \n",
      "Epoch 1 | 3200 | Current loss 0.14043734967708588 \n",
      "Epoch 1 | 3300 | Current loss 0.022034797817468643 \n",
      "Epoch 1 | 3400 | Current loss 0.24854207038879395 \n",
      "Epoch 1 | 3500 | Current loss 0.07558410614728928 \n",
      "Epoch 1 | 3600 | Current loss 0.08793479204177856 \n",
      "Epoch 1 | 3700 | Current loss 0.4044596254825592 \n",
      "Epoch 1 | 3800 | Current loss 0.14799755811691284 \n",
      "Epoch 1 | 3900 | Current loss 0.047361090779304504 \n",
      "Epoch 1 | 4000 | Current loss 0.17394036054611206 \n",
      "Epoch 1 | 4100 | Current loss 0.18945571780204773 \n",
      "Epoch 1 | 4200 | Current loss 0.4175646901130676 \n",
      "Epoch 1 | 4300 | Current loss 0.17162664234638214 \n",
      "Epoch 1 | 4400 | Current loss 0.1539938598871231 \n",
      "Epoch 1 | 4500 | Current loss 0.02158479392528534 \n",
      "Epoch 1 | 4600 | Current loss 0.19491669535636902 \n",
      "Epoch 1 | 4700 | Current loss 0.3115837574005127 \n",
      "Epoch 1 | 4800 | Current loss 0.0811263844370842 \n",
      "Epoch 1 | 4900 | Current loss 0.08649478107690811 \n",
      "Epoch 1 | 5000 | Current loss 0.13329565525054932 \n",
      "Epoch 1 | 5100 | Current loss 0.22676971554756165 \n",
      "Epoch 1 | 5200 | Current loss 0.0317312628030777 \n",
      "Epoch 1 | 5300 | Current loss 0.0980539470911026 \n",
      "Epoch 1 | 5400 | Current loss 0.13073626160621643 \n",
      "Epoch 1 | 5500 | Current loss 0.16421568393707275 \n",
      "Epoch 1 | 5600 | Current loss 0.09587272256612778 \n",
      "Epoch 1 | 5700 | Current loss 0.24231573939323425 \n",
      "Epoch 1 | 5800 | Current loss 0.1176590770483017 \n",
      "Epoch 1 | 5900 | Current loss 0.10965737700462341 \n",
      "Epoch 1 | 6000 | Current loss 0.15715456008911133 \n",
      "Epoch 1 | 6100 | Current loss 0.23502367734909058 \n",
      "Epoch 1 | 6200 | Current loss 0.10573025792837143 \n",
      "Epoch 1 | 6300 | Current loss 0.07827654480934143 \n",
      "Epoch 1 | 6400 | Current loss 0.08841034770011902 \n",
      "Epoch 1 | 6500 | Current loss 0.3712201416492462 \n",
      "Epoch 1 | 6600 | Current loss 0.06246967241168022 \n",
      "Epoch 1 | 6700 | Current loss 0.2004912793636322 \n",
      "Epoch 1 | 6800 | Current loss 0.17156803607940674 \n",
      "Epoch 1 | 6900 | Current loss 0.28251633048057556 \n",
      "Epoch 1 | 7000 | Current loss 0.1376764476299286 \n",
      "Epoch 1 | 7100 | Current loss 0.32967907190322876 \n",
      "Epoch 1 | 7200 | Current loss 0.20439228415489197 \n",
      "Epoch 1 | 7300 | Current loss 0.06312714517116547 \n",
      "Epoch 1 | 7400 | Current loss 0.11148172616958618 \n",
      "Epoch 1 | 7500 | Current loss 0.27764663100242615 \n",
      "Epoch 1 | 7600 | Current loss 0.056612640619277954 \n",
      "Epoch 1 | 7700 | Current loss 0.2110988199710846 \n",
      "Epoch 1 | 7800 | Current loss 0.14111874997615814 \n",
      "Epoch 1 | 7900 | Current loss 0.1122199296951294 \n",
      "Epoch 1 | 8000 | Current loss 0.27933958172798157 \n",
      "Epoch 1 | 8100 | Current loss 0.11730585992336273 \n",
      "Epoch 1 | 8200 | Current loss 0.1914294958114624 \n",
      "Epoch 1 | 8300 | Current loss 0.09403431415557861 \n",
      "Epoch 1 | 8400 | Current loss 0.07823324203491211 \n",
      "Epoch 1 | 8500 | Current loss 0.11724981665611267 \n",
      "Epoch 1 | 8600 | Current loss 0.12725727260112762 \n",
      "Epoch 1 | 8700 | Current loss 0.09853152930736542 \n",
      "Epoch 1 | 8800 | Current loss 0.05965040996670723 \n",
      "Epoch 1 | 8900 | Current loss 0.10423646867275238 \n",
      "Epoch 1 | 9000 | Current loss 0.28260812163352966 \n",
      "Epoch 1 | 9100 | Current loss 0.035681482404470444 \n",
      "Epoch 1 | 9200 | Current loss 0.04287859424948692 \n",
      "Epoch 1 | 9300 | Current loss 0.1990494728088379 \n",
      "Epoch 1 | 9400 | Current loss 0.15860024094581604 \n",
      "Epoch 1 | 9500 | Current loss 0.17636504769325256 \n",
      "Epoch 1 | 9600 | Current loss 0.16185162961483002 \n",
      "Epoch 1 | 9700 | Current loss 0.26104727387428284 \n",
      "Epoch 1 | 9800 | Current loss 0.1267940104007721 \n",
      "Epoch 1 | 9900 | Current loss 0.21873274445533752 \n",
      "Epoch 1 | 10000 | Current loss 0.10299687087535858 \n",
      "Epoch 1 | 10100 | Current loss 0.1811075657606125 \n",
      "Epoch 1 | 10200 | Current loss 0.12617051601409912 \n",
      "Epoch 1 | 10300 | Current loss 0.23644082248210907 \n",
      "Epoch 1 | 10400 | Current loss 0.14077404141426086 \n",
      "Epoch 1 | 10500 | Current loss 0.30330023169517517 \n",
      "Epoch 1 | 10600 | Current loss 0.0679556280374527 \n",
      "Epoch 1 | 10700 | Current loss 0.12137935310602188 \n",
      "Epoch 1 | 10800 | Current loss 0.18969598412513733 \n",
      "Epoch 1 | 10900 | Current loss 0.13628040254116058 \n",
      "Epoch 1 | 11000 | Current loss 0.14809222519397736 \n",
      "Epoch 1 | 11100 | Current loss 0.0735686793923378 \n",
      "Epoch 1 | 11200 | Current loss 0.17469249665737152 \n",
      "Epoch 1 | 11300 | Current loss 0.17946019768714905 \n",
      "Epoch 1 | 11400 | Current loss 0.1909155696630478 \n",
      "Epoch 1 | 11500 | Current loss 0.08009354770183563 \n",
      "Epoch 1 | 11600 | Current loss 0.10702717304229736 \n",
      "Epoch 1 | 11700 | Current loss 0.18207409977912903 \n",
      "Epoch 1 | 11800 | Current loss 0.10245376825332642 \n",
      "Epoch 1 | 11900 | Current loss 0.13609591126441956 \n",
      "Epoch 1 | 12000 | Current loss 0.06653927266597748 \n",
      "Epoch 1 | 12100 | Current loss 0.15690433979034424 \n",
      "Epoch 1 | 12200 | Current loss 0.036727532744407654 \n",
      "Epoch 1 | 12300 | Current loss 0.07805503904819489 \n",
      "Epoch 1 | 12400 | Current loss 0.09199506789445877 \n",
      "Epoch 1 | 12500 | Current loss 0.11158391833305359 \n",
      "Epoch 1 | 12600 | Current loss 0.2798447906970978 \n",
      "Epoch 1 | 12700 | Current loss 0.4011912941932678 \n",
      "Epoch 1 | 12800 | Current loss 0.16688552498817444 \n",
      "Epoch 1 | 12900 | Current loss 0.14547038078308105 \n",
      "Epoch 1 | 13000 | Current loss 0.11182601004838943 \n",
      "Epoch 1 | 13100 | Current loss 0.16193607449531555 \n",
      "Epoch 1 | 13200 | Current loss 0.22296196222305298 \n",
      "Epoch 1 | 13300 | Current loss 0.2664819359779358 \n",
      "Epoch 1 | 13400 | Current loss 0.17977917194366455 \n",
      "Epoch 1 | 13500 | Current loss 0.1019183024764061 \n",
      "Epoch 1 | 13600 | Current loss 0.24654027819633484 \n",
      "Epoch 1 | 13700 | Current loss 0.1760905385017395 \n",
      "Epoch 1 | 13800 | Current loss 0.1241324245929718 \n",
      "Epoch 1 | 13900 | Current loss 0.11008954048156738 \n",
      "Epoch 1 | 14000 | Current loss 0.22205087542533875 \n",
      "Epoch 1 | 14100 | Current loss 0.10191456973552704 \n",
      "Epoch 1 | 14200 | Current loss 0.10889260470867157 \n",
      "Epoch 1 | 14300 | Current loss 0.09149757027626038 \n",
      "Epoch 1 | 14400 | Current loss 0.2685858905315399 \n",
      "Epoch 1 | 14500 | Current loss 0.11513546109199524 \n",
      "Epoch 1 | 14600 | Current loss 0.08478371053934097 \n",
      "Epoch 1 | 14700 | Current loss 0.43999382853507996 \n",
      "Epoch 1 | 14800 | Current loss 0.07766146957874298 \n",
      "Epoch 1 | 14900 | Current loss 0.10689888894557953 \n",
      "Epoch 1 | 15000 | Current loss 0.13834348320960999 \n",
      "Epoch 1 | 15100 | Current loss 0.16116468608379364 \n",
      "Epoch 1 | 15200 | Current loss 0.041763558983802795 \n",
      "Epoch 1 | 15300 | Current loss 0.07486151158809662 \n",
      "Epoch 1 | 15400 | Current loss 0.06365048885345459 \n",
      "Epoch 1 | 15500 | Current loss 0.12231630086898804 \n",
      "Epoch 1 | 15600 | Current loss 0.11343144625425339 \n",
      "Epoch 1 | 15700 | Current loss 0.14852263033390045 \n",
      "Epoch 1 | 15800 | Current loss 0.14178365468978882 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | 15900 | Current loss 0.043923527002334595 \n",
      "Epoch 1 | 16000 | Current loss 0.0938175618648529 \n",
      "Epoch 1 | 16100 | Current loss 0.10586828738451004 \n",
      "Epoch 1 | 16200 | Current loss 0.11813624203205109 \n",
      "Epoch 1 | 16300 | Current loss 0.12666547298431396 \n",
      "Epoch 1 | 16400 | Current loss 0.18063274025917053 \n",
      "Epoch 1 | 16500 | Current loss 0.15545512735843658 \n",
      "Epoch 1 | 16600 | Current loss 0.05605354160070419 \n",
      "Epoch 1 | 16700 | Current loss 0.03743017464876175 \n",
      "Epoch 1 | 16800 | Current loss 0.25675809383392334 \n",
      "Epoch 1 | 16900 | Current loss 0.2754192054271698 \n",
      "Epoch 1 | 17000 | Current loss 0.3047811686992645 \n",
      "Epoch 1 | 17100 | Current loss 0.0986749604344368 \n",
      "Epoch 1 | 17200 | Current loss 0.07563415914773941 \n",
      "Epoch 1 | 17300 | Current loss 0.31965866684913635 \n",
      "Epoch 1 | 17400 | Current loss 0.2746994197368622 \n",
      "Epoch 1 | 17500 | Current loss 0.1601133644580841 \n",
      "Epoch 1 | 17600 | Current loss 0.02849753573536873 \n",
      "Epoch 1 | 17700 | Current loss 0.22808022797107697 \n",
      "Epoch 1 | 17800 | Current loss 0.15894688665866852 \n",
      "Epoch 1 | 17900 | Current loss 0.08170486241579056 \n",
      "Epoch 1 | 18000 | Current loss 0.15442690253257751 \n",
      "Epoch 1 | 18100 | Current loss 0.08720255643129349 \n",
      "Epoch 1 | 18200 | Current loss 0.0818571075797081 \n",
      "Epoch 1 | 18300 | Current loss 0.18435706198215485 \n",
      "Epoch 1 | 18400 | Current loss 0.0550239235162735 \n",
      "Epoch 1 | 18500 | Current loss 0.34378165006637573 \n",
      "Epoch 1 | 18600 | Current loss 0.12927648425102234 \n",
      "Epoch 1 | 18700 | Current loss 0.16984106600284576 \n",
      "Epoch 1 | 18800 | Current loss 0.11828875541687012 \n",
      "Epoch 1 | 18900 | Current loss 0.034594401717185974 \n",
      "Epoch 1 | 19000 | Current loss 0.030128775164484978 \n",
      "Epoch 1 | 19100 | Current loss 0.0651036947965622 \n",
      "Epoch 1 | 19200 | Current loss 0.09485496580600739 \n",
      "Epoch 1 | 19300 | Current loss 0.07275953888893127 \n",
      "Epoch 1 | 19400 | Current loss 0.12280721962451935 \n",
      "Epoch 1 | 19500 | Current loss 0.11003180593252182 \n",
      "Epoch 1 | 19600 | Current loss 0.12809598445892334 \n",
      "Epoch 1 | 19700 | Current loss 0.19149252772331238 \n",
      "Epoch 1 | 19800 | Current loss 0.24017943441867828 \n",
      "Epoch 1 | 19900 | Current loss 0.3007858693599701 \n",
      "Epoch 1 | 20000 | Current loss 0.2880467176437378 \n",
      "Epoch 1 | 20100 | Current loss 0.15190596878528595 \n",
      "Epoch 1 | 20200 | Current loss 0.08647841960191727 \n",
      "Epoch 1 | 20300 | Current loss 0.27633529901504517 \n",
      "Epoch 1 | 20400 | Current loss 0.47885701060295105 \n",
      "Epoch 1 | 20500 | Current loss 0.040392011404037476 \n",
      "Epoch 1 | 20600 | Current loss 0.07776892930269241 \n",
      "Epoch 1 | 20700 | Current loss 0.25201496481895447 \n",
      "Epoch 1 | 20800 | Current loss 0.11828907579183578 \n",
      "Epoch 1 | 20900 | Current loss 0.2626299560070038 \n",
      "Epoch 1 | 21000 | Current loss 0.0861680880188942 \n",
      "Epoch 1 | 21100 | Current loss 0.11264821887016296 \n",
      "Epoch 1 | 21200 | Current loss 0.06921513378620148 \n",
      "Epoch 1 | 21300 | Current loss 0.11016588658094406 \n",
      "Epoch 1 | 21400 | Current loss 0.07767599821090698 \n",
      "Epoch 1 | 21500 | Current loss 0.31614989042282104 \n",
      "Epoch 1 | 21600 | Current loss 0.19910618662834167 \n",
      "Epoch 1 | 21700 | Current loss 0.18307891488075256 \n",
      "Epoch 1 | 21800 | Current loss 0.10740714520215988 \n",
      "Epoch 1 | 21900 | Current loss 0.19135549664497375 \n",
      "Epoch 1 | 22000 | Current loss 0.04835229367017746 \n",
      "Epoch 1 | 22100 | Current loss 0.0395437516272068 \n",
      "Epoch 1 | 22200 | Current loss 0.15970274806022644 \n",
      "Epoch 1 | 22300 | Current loss 0.14651355147361755 \n",
      "Epoch 1 | 22400 | Current loss 0.290609210729599 \n",
      "Epoch 1 | 22500 | Current loss 0.12866419553756714 \n",
      "Epoch 1 | 22600 | Current loss 0.1500939130783081 \n",
      "Epoch 1 | 22700 | Current loss 0.21874099969863892 \n",
      "Epoch 1 | 22800 | Current loss 0.15948700904846191 \n",
      "Epoch 1 | 22900 | Current loss 0.1072382926940918 \n",
      "Epoch 1 | 23000 | Current loss 0.11650917679071426 \n",
      "Epoch 1 | 23100 | Current loss 0.04924114793539047 \n",
      "Epoch 1 | 23200 | Current loss 0.049183234572410583 \n",
      "Epoch 1 | 23300 | Current loss 0.05647236853837967 \n",
      "Epoch 1 | 23400 | Current loss 0.20003914833068848 \n",
      "Epoch 1 | 23500 | Current loss 0.1244259923696518 \n",
      "Epoch 1 | 23600 | Current loss 0.13596202433109283 \n",
      "Epoch 1 | 23700 | Current loss 0.1568746715784073 \n",
      "Epoch 1 | 23800 | Current loss 0.17688900232315063 \n",
      "Epoch 1 | 23900 | Current loss 0.018634241074323654 \n",
      "Epoch 1 | 24000 | Current loss 0.17435847222805023 \n",
      "Epoch 1 | 24100 | Current loss 0.0724804550409317 \n",
      "Epoch 1 | 24200 | Current loss 0.10234301537275314 \n",
      "Epoch 1 | 24300 | Current loss 0.16900409758090973 \n",
      "Epoch 1 | 24400 | Current loss 0.11004982143640518 \n",
      "Epoch 1 | 24500 | Current loss 0.04317415505647659 \n",
      "Epoch 1 | 24600 | Current loss 0.05490728095173836 \n",
      "Epoch 1 | 24700 | Current loss 0.04508742690086365 \n",
      "Epoch 1 | 24800 | Current loss 0.16636323928833008 \n",
      "Epoch 1 | 24900 | Current loss 0.21205756068229675 \n",
      "Epoch 1 | 25000 | Current loss 0.272405743598938 \n",
      "Epoch 1 | 25100 | Current loss 0.22822238504886627 \n",
      "Epoch 1 | 25200 | Current loss 0.02871280163526535 \n",
      "Epoch 1 | 25300 | Current loss 0.12534326314926147 \n",
      "Epoch 1 | 25400 | Current loss 0.08671039342880249 \n",
      "Epoch 1 | 25500 | Current loss 0.0951153039932251 \n",
      "Epoch 1 | 25600 | Current loss 0.09889255464076996 \n",
      "Epoch 1 | 25700 | Current loss 0.1547778844833374 \n",
      "Epoch 1 | 25800 | Current loss 0.028576547279953957 \n",
      "Epoch 1 | 25900 | Current loss 0.16559645533561707 \n",
      "Epoch 1 | 26000 | Current loss 0.15601211786270142 \n",
      "Epoch 1 | 26100 | Current loss 0.1698978692293167 \n",
      "Epoch 1 | 26200 | Current loss 0.23670081794261932 \n",
      "Epoch 1 | 26300 | Current loss 0.09104801714420319 \n",
      "Epoch 1 | 26400 | Current loss 0.13576313853263855 \n",
      "Epoch 1 | 26500 | Current loss 0.2873871624469757 \n",
      "Epoch 1 | 26600 | Current loss 0.17012333869934082 \n",
      "Epoch 1 | 26700 | Current loss 0.10807794332504272 \n",
      "Epoch 1 | 26800 | Current loss 0.11484396457672119 \n",
      "Epoch 1 | 26900 | Current loss 0.1678672730922699 \n",
      "Epoch 1 | 27000 | Current loss 0.07749523222446442 \n",
      "Epoch 1 | 27100 | Current loss 0.044909145683050156 \n",
      "Epoch 1 | 27200 | Current loss 0.07053782045841217 \n",
      "Epoch 1 | 27300 | Current loss 0.2329096496105194 \n",
      "Epoch 1 | 27400 | Current loss 0.34614747762680054 \n",
      "Epoch 1 | 27500 | Current loss 0.08595098555088043 \n",
      "Epoch 1 | 27600 | Current loss 0.08640358597040176 \n",
      "Epoch 1 | 27700 | Current loss 0.20450210571289062 \n",
      "Epoch 1 | 27800 | Current loss 0.18135285377502441 \n",
      "Epoch 1 | 27900 | Current loss 0.04182100296020508 \n",
      "Epoch 1 | 28000 | Current loss 0.15219512581825256 \n",
      "Epoch 1 | 28100 | Current loss 0.072477325797081 \n",
      "Epoch 1 | 28200 | Current loss 0.1719662845134735 \n",
      "Epoch 1 | 28300 | Current loss 0.016976384446024895 \n",
      "Epoch 1 | 28400 | Current loss 0.21991102397441864 \n",
      "Epoch 1 | 28500 | Current loss 0.1485065072774887 \n",
      "Epoch 1 | 28600 | Current loss 0.24631425738334656 \n",
      "Epoch 1 | 28700 | Current loss 0.2612876296043396 \n",
      "Epoch 1 | 28800 | Current loss 0.23832213878631592 \n",
      "Epoch 1 | 28900 | Current loss 0.04176429659128189 \n",
      "Epoch 1 | 29000 | Current loss 0.05064091458916664 \n",
      "Epoch 1 | 29100 | Current loss 0.15739697217941284 \n",
      "Epoch 1 | 29200 | Current loss 0.04054839164018631 \n",
      "Epoch 1 | 29300 | Current loss 0.1735636293888092 \n",
      "Epoch 1 | 29400 | Current loss 0.14945422112941742 \n",
      "Epoch 1 | 29500 | Current loss 0.19839753210544586 \n",
      "Epoch 1 | 29600 | Current loss 0.1928752362728119 \n",
      "Epoch 1 | 29700 | Current loss 0.08438753336668015 \n",
      "Epoch 1 | 29800 | Current loss 0.1953328251838684 \n",
      "Epoch 1 | 29900 | Current loss 0.1379542350769043 \n",
      "Epoch 1 | 30000 | Current loss 0.09551658481359482 \n",
      "Epoch 1 | 30100 | Current loss 0.13170646131038666 \n",
      "Epoch 1 | 30200 | Current loss 0.2666717767715454 \n",
      "Epoch 1 | 30300 | Current loss 0.10972994565963745 \n",
      "Epoch 1 | 30400 | Current loss 0.17997851967811584 \n",
      "Epoch 1 | 30500 | Current loss 0.0830998569726944 \n",
      "Epoch 1 | 30600 | Current loss 0.0666346549987793 \n",
      "Epoch 1 | 30700 | Current loss 0.3157205581665039 \n",
      "Epoch 1 | 30800 | Current loss 0.22749435901641846 \n",
      "Epoch 1 | 30900 | Current loss 0.11791611462831497 \n",
      "Epoch 1 | 31000 | Current loss 0.01755780726671219 \n",
      "Epoch 1 | 31100 | Current loss 0.09222535789012909 \n",
      "Epoch 1 | 31200 | Current loss 0.09919753670692444 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:24<00:00,  3.82s/it]\n",
      "100%|██████████| 100/100 [06:31<00:00,  4.06s/it]\n",
      "100%|██████████| 100/100 [06:26<00:00,  3.83s/it]\n",
      "100%|██████████| 100/100 [06:39<00:00,  3.95s/it]\n",
      "100%|██████████| 100/100 [06:33<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | acc_top20: 0.908 | acc_top50: 0.944 | acc_top100: 0.956 | acc_top200: 0.978 | acc_mean: 0.946\n",
      "Epoch 2 | 0 | Current loss 0.13576364517211914 \n",
      "Epoch 2 | 100 | Current loss 0.11287466436624527 \n",
      "Epoch 2 | 200 | Current loss 0.14667025208473206 \n",
      "Epoch 2 | 300 | Current loss 0.22872218489646912 \n",
      "Epoch 2 | 400 | Current loss 0.11244642734527588 \n",
      "Epoch 2 | 500 | Current loss 0.160735622048378 \n",
      "Epoch 2 | 600 | Current loss 0.04096594825387001 \n",
      "Epoch 2 | 700 | Current loss 0.020638512447476387 \n",
      "Epoch 2 | 800 | Current loss 0.14797013998031616 \n",
      "Epoch 2 | 900 | Current loss 0.09647265821695328 \n",
      "Epoch 2 | 1000 | Current loss 0.17924925684928894 \n",
      "Epoch 2 | 1100 | Current loss 0.05210963264107704 \n",
      "Epoch 2 | 1200 | Current loss 0.1776515543460846 \n",
      "Epoch 2 | 1300 | Current loss 0.07134465873241425 \n",
      "Epoch 2 | 1400 | Current loss 0.09209587424993515 \n",
      "Epoch 2 | 1500 | Current loss 0.39818209409713745 \n",
      "Epoch 2 | 1600 | Current loss 0.14826036989688873 \n",
      "Epoch 2 | 1700 | Current loss 0.20689073204994202 \n",
      "Epoch 2 | 1800 | Current loss 0.014921926893293858 \n",
      "Epoch 2 | 1900 | Current loss 0.2375069260597229 \n",
      "Epoch 2 | 2000 | Current loss 0.18289071321487427 \n",
      "Epoch 2 | 2100 | Current loss 0.23091495037078857 \n",
      "Epoch 2 | 2200 | Current loss 0.0274665504693985 \n",
      "Epoch 2 | 2300 | Current loss 0.09084340929985046 \n",
      "Epoch 2 | 2400 | Current loss 0.16594848036766052 \n",
      "Epoch 2 | 2500 | Current loss 0.2588500380516052 \n",
      "Epoch 2 | 2600 | Current loss 0.13588681817054749 \n",
      "Epoch 2 | 2700 | Current loss 0.2253643125295639 \n",
      "Epoch 2 | 2800 | Current loss 0.19587263464927673 \n",
      "Epoch 2 | 2900 | Current loss 0.2859308123588562 \n",
      "Epoch 2 | 3000 | Current loss 0.18800045549869537 \n",
      "Epoch 2 | 3100 | Current loss 0.09662619233131409 \n",
      "Epoch 2 | 3200 | Current loss 0.09820058941841125 \n",
      "Epoch 2 | 3300 | Current loss 0.1287364363670349 \n",
      "Epoch 2 | 3400 | Current loss 0.031146705150604248 \n",
      "Epoch 2 | 3500 | Current loss 0.11528751254081726 \n",
      "Epoch 2 | 3600 | Current loss 0.10999281704425812 \n",
      "Epoch 2 | 3700 | Current loss 0.16753004491329193 \n",
      "Epoch 2 | 3800 | Current loss 0.09786873310804367 \n",
      "Epoch 2 | 3900 | Current loss 0.20068714022636414 \n",
      "Epoch 2 | 4000 | Current loss 0.10377383232116699 \n",
      "Epoch 2 | 4100 | Current loss 0.12055785953998566 \n",
      "Epoch 2 | 4200 | Current loss 0.08475092053413391 \n",
      "Epoch 2 | 4300 | Current loss 0.02471170946955681 \n",
      "Epoch 2 | 4400 | Current loss 0.018418148159980774 \n",
      "Epoch 2 | 4500 | Current loss 0.06593294441699982 \n",
      "Epoch 2 | 4600 | Current loss 0.1370561420917511 \n",
      "Epoch 2 | 4700 | Current loss 0.07865478098392487 \n",
      "Epoch 2 | 4800 | Current loss 0.15508244931697845 \n",
      "Epoch 2 | 4900 | Current loss 0.22530069947242737 \n",
      "Epoch 2 | 5000 | Current loss 0.2527078688144684 \n",
      "Epoch 2 | 5100 | Current loss 0.09924059361219406 \n",
      "Epoch 2 | 5200 | Current loss 0.28107529878616333 \n",
      "Epoch 2 | 5300 | Current loss 0.13203690946102142 \n",
      "Epoch 2 | 5400 | Current loss 0.14497867226600647 \n",
      "Epoch 2 | 5500 | Current loss 0.27229005098342896 \n",
      "Epoch 2 | 5600 | Current loss 0.0698547288775444 \n",
      "Epoch 2 | 5700 | Current loss 0.11870444566011429 \n",
      "Epoch 2 | 5800 | Current loss 0.07940026372671127 \n",
      "Epoch 2 | 5900 | Current loss 0.08210901916027069 \n",
      "Epoch 2 | 6000 | Current loss 0.13941700756549835 \n",
      "Epoch 2 | 6100 | Current loss 0.050364233553409576 \n",
      "Epoch 2 | 6200 | Current loss 0.1440378576517105 \n",
      "Epoch 2 | 6300 | Current loss 0.02443687617778778 \n",
      "Epoch 2 | 6400 | Current loss 0.043528951704502106 \n",
      "Epoch 2 | 6500 | Current loss 0.11820542812347412 \n",
      "Epoch 2 | 6600 | Current loss 0.01803438737988472 \n",
      "Epoch 2 | 6700 | Current loss 0.1082809641957283 \n",
      "Epoch 2 | 6800 | Current loss 0.48024335503578186 \n",
      "Epoch 2 | 6900 | Current loss 0.21264871954917908 \n",
      "Epoch 2 | 7000 | Current loss 0.03678613901138306 \n",
      "Epoch 2 | 7100 | Current loss 0.07999765872955322 \n",
      "Epoch 2 | 7200 | Current loss 0.06345587968826294 \n",
      "Epoch 2 | 7300 | Current loss 0.294760525226593 \n",
      "Epoch 2 | 7400 | Current loss 0.10691173374652863 \n",
      "Epoch 2 | 7500 | Current loss 0.11329023540019989 \n",
      "Epoch 2 | 7600 | Current loss 0.22739361226558685 \n",
      "Epoch 2 | 7700 | Current loss 0.14756572246551514 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,15):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"./checkpoint/ckpt.2019-04-24 18:24:29.0.96.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line probability show\n",
    "folder_dataset_test = SkylineDataset(root=Config.testing_dir, \n",
    "                                transform=transforms.Compose([ToTensor()]))\n",
    "test_dataloader = DataLoader(folder_dataset_test,num_workers=6,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "example0 = next(dataiter)\n",
    "line0 = example0[\"line\"][0].view(1, 1, -1)\n",
    "line1 = example0[\"line\"][1].view(1, 1, -1)\n",
    "label = example0[\"label\"].numpy()\n",
    "print(label)\n",
    "for i in range(10):\n",
    "    output = model(Variable(line0).to(device),Variable(line1).to(device))\n",
    "    line_show_test((line0, line1, label),'probability: {:.8f}'.format(output.item()))\n",
    "    \n",
    "    example1 = next(dataiter)\n",
    "    line1 = example1[\"line\"][1].view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc inference\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    filenames = os.listdir(Config.testing_dir)\n",
    "    acc_top5 = 0\n",
    "    acc_top10 = 0\n",
    "    acc_top20 = 0\n",
    "    acc_top50 = 0\n",
    "    acc_top100 = 0\n",
    "    acc_top200 = 0\n",
    "\n",
    "    for i in trange(100):\n",
    "        index_target = random.randint(0, len(filenames)-1)\n",
    "        file0 = read_data_row(os.path.join(Config.testing_dir, filenames[index_target]), 1).strip().split(\" \")\n",
    "        line0 = np.array(list(map(int, file0[0].split(','))))\n",
    "        dist = []\n",
    "        for f in filenames:\n",
    "            file1 = read_data_row(os.path.join(Config.testing_dir, f), 1).strip().split(\" \")\n",
    "            line1 = np.array(list(map(int, file1[1].split(','))))\n",
    "            line = np.hstack((line0, line1))\n",
    "            line_min, line_max = line.min(), line.max()\n",
    "            line = (line-line_min)/(line_max-line_min)\n",
    "            line1 = torch.from_numpy(line[:300].reshape(1, 1, -1))\n",
    "            line2 = torch.from_numpy(line[300:].reshape(1, 1, -1))\n",
    "\n",
    "            output = model(Variable(line1).float().to(device), Variable(line2).float().to(device))\n",
    "            dist.append(output.item())\n",
    "\n",
    "        dist = pd.Series(np.array(dist)).sort_values(ascending=False)\n",
    "        if i == 0:\n",
    "            print(index_target)\n",
    "            print(dist)\n",
    "        if index_target in dist[:5].index.tolist():\n",
    "            acc_top5 += 1\n",
    "        if index_target in dist[:10].index.tolist():\n",
    "            acc_top10 += 1    \n",
    "        if index_target in dist[:20].index.tolist():\n",
    "            acc_top20 += 1\n",
    "        if index_target in dist[:50].index.tolist():\n",
    "            acc_top50 += 1    \n",
    "        if index_target in dist[:100].index.tolist():\n",
    "            acc_top100 += 1\n",
    "        if index_target in dist[:200].index.tolist():\n",
    "            acc_top200 += 1\n",
    "\n",
    "    \n",
    "    acc_top5 /= 100.0\n",
    "    acc_top10 /= 100.0\n",
    "    acc_top20 /= 100.0\n",
    "    acc_top50 /= 100.0\n",
    "    acc_top100 /= 100.0\n",
    "    acc_top200 /= 100.0\n",
    "    acc_mean = (acc_top5+acc_top10+acc_top20+acc_top50+acc_top100+acc_top200) / 6\n",
    "    pf = 'acc_top5: %.3f |acc_top10: %.3f |acc_top20: %.3f | acc_top50: %.3f | acc_top100: %.3f | acc_top200: %.3f | acc_mean: %.3f'\n",
    "    print(pf % ( acc_top5,\n",
    "                 acc_top10,\n",
    "                 acc_top20, \n",
    "                 acc_top50,\n",
    "                 acc_top100,\n",
    "                 acc_top200,\n",
    "                 acc_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
